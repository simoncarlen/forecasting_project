\section{Ambient air pollution}

Ambient air pollution is one of the greatest environmental and health concerns of the modern world. Worldwide, poor air quality causes millions of premature deaths every year and is linked to several adverse health effects such as respiratory problems, cardiovascular disease, and cancer \cite{who2016}. In addition to health risks, the global economic impacts are substantial due to lost labor productivity, increased health care costs, reduced crop yields, etc.\ \cite{oecd2016}. Outdoor air pollution has become a ubiquitous problem, affecting both cities and rural areas, and it is estimated that about 90\% of the world's population are living in regions where air pollution levels exceed guidelines set by the World Health Organization (WHO) \cite{who2016}. 

\subsection{Principal air pollutants}
\label{sec:airpollutants}
In densely populated urban areas, air pollution levels can periodically be severe, and with an accelerating urbanization, it has become imperative for public health authorities to closely monitor city air and try to mitigate the harmful effects of pollution. Commonly monitored substances include sulphur dioxide (SO$_2$), nitrogen oxides (NO$_x$, i.e., NO and NO$_2$), carbon monoxide (CO), ground-level ozone (O$_3$), volatile organic compounds (VOCs), and particulate matter (PM) \cite{VanLoon2010}. Vehicular traffic is a major source of the gaseous pollutants NO$_x$, SO$_2$, CO, and VOCs, but certain industrial processes also contribute to emissions \cite{VanLoon2010}. Ground-level O$_3$ (also a gas) is a so-called secondary pollutant that forms when NO$_x$ and VOCs react on sunny days with little wind \cite{VanLoon2010}. PM are atmospheric aerosol particles (particles suspended in the air). They have diverse origins, both natural and anthropogenic, and a complex chemical composition consisting of both solid and liquid species \cite{VanLoon2010}.

%\paragraph{Particulate matter}
%PM, the group of pollutants being the focus of this work, are atmospheric aerosol particles (i.e., particles suspended in the air). They have diverse origins, both natural and anthropogenic, and a complex chemical composition consisting of both solid and liquid species \cite{Schwarzenbach2016}. Some important sources of PM are forest fires, volcanic eruptions, sand/dust storms, sea spray, vehicular traffic, certain industrial processes, construction sites, and domestic combustion \cite{Querol2004, Schwarzenbach2016}. When entering the atmosphere directly by these routes, one denotes the PM as primary. However, PM can also be formed by the oxidation of gases such as SO$_2$, NO$_x$, and VOCs (followed by a complex chemical reaction process), in which case the PM is said to be secondary \cite{Schwarzenbach2016}. PM is also categorized by particle size (or more specifically, the aerodynamic diameter), and particles measuring smaller than 2.5 \textmugreek m and 10 \textmugreek m are denoted as PM$_{2.5}$ and PM$_{10}$, respectively \cite{Schwarzenbach2016}.

%Both PM$_{10}$ and PM$_{2.5}$ can travel long distances from point sources (though PM$_{2.5}$ has a longer residence time in the atmosphere than PM$_{10}$), and local pollution can be affected by regional background levels \cite{Schwarzenbach2016, slbanalys}. PM levels are also dependent on weather conditions \cite{Schwarzenbach2016}. For example, temperature and solar radiation are related to the formation of secondary PM, and PM emission from roads, tires, brake wear, etc., can be affected by precipitation and humidity \cite{slbanalys, atmos7020015}. Both PM$_{10}$ and PM$_{2.5}$ are hazardous and cause a wide range of health problems, though PM$_{2.5}$ can more easily penetrate the lungs \cite{Schwarzenbach2016}. In the European Union, annual mean limits are set to 40 \textmugreek g/m$^3$ for PM$_{10}$ and 20 \textmugreek g/m$^3$ for PM$_{2.5}$ \cite{eu-airquality}. 

\subsection{Ambient air pollution in Stockholm}
\label{air-pollution-stockholm}

In the city of Stockholm, environmental air quality standards are usually met, though some streets experience occasional episodes with severe pollution levels (e.g. Hornsgatan is one such street) \cite{slbanalys2021}. Since Stockholm has centralized district heating and few industries, the major source of local CO, NO$_x$, and PM pollution is vehicular traffic \cite{slbanalys, slbanalys2021}. Mechanical wear by studded tires on asphalt and the wearing of brakes and tiers in motor vehicles contribute substantially to local levels of both PM$_{10}$ and PM$_{2.5}$. For PM$_{2.5}$ however, contribution from sources outside of Stockholm is also significant \cite{slbanalys2021}. Emission of SO$_2$ can come from the energy sector and waterborne transport, though local levels are also affected by outside sources. 
%The levels of SO$_2$ are affected by transport from outside sources, though local and regional emissions can be due to the energy sector and waterborne transport \cite{slbanalys2021}. 
For O$_3$, long-range transport from mainland Europe is the single-most important factor contributing to locally measured levels \cite{slbanalys2021}. 

The air in Stockholm County is monitored by Stockholms Luft- och Bulleranalys (SLB-analys), a unit in the Environment and Health Administration (EHA) of the city of Stockholm. SLB-analys are responsible for a number of monitoring stations measuring several air pollutants and some meteorological parameters in the Stockholm region, as well as a few stations outside of Stockholm \cite{slb-matningar}. In addition to monitoring the air, SLB-analys also model and forecast air pollution levels for the Stockholm metropolitan area, and their forecasts are available through a smartphone application, called "Luft i Stockholm" \cite{slbanalys}. 

\section{Forecasting air pollution}

Having the possibility to forecast air pollution levels hours or days ahead can be extremely valuable to regulatory authorities in order to protect public health, and vulnerable groups in particular. In general, there are two broad categories of models for such forecasts; mechanistic models, and statistical and/or machine learning models \cite{ElHarbawi2013}. This work is concerned with the latter type, and in the sections below a review follows. The mathematical and statistical theory behind many of the models is quite extensive \cite{Hastie2009, Montgomery2015, smlbook, LeCun2015}, but relevant theory will be covered briefly.

\subsection{Forecasting as a regression problem}
\label{sec:forecasting}
While mechanistic models are based on mathematical modelling of atmospheric processes along with other factors governing the distribution of air pollution (such as emission source characteristics, physico-chemical properties of pollutants, terrain and building design, etc.) statistical and/or machine learning models are entirely data-driven, being derived directly from measurements on the variables of interest \cite{ElHarbawi2013}. 

From a statistical learning perspective, forecasting air pollution can be viewed as a regression problem, in which a function $f$, mapping input data to a numerical output, is being approximated (or learned) from a training set of labeled input-output examples \cite{smlbook}. Learning the function $f$ amounts to finding a set of parameters (or weights/coefficients) for the model, which in the case of a simpler regression technique can be only a handful, but possibly millions if a deep neural network is used \cite{LeCun2015}. Generally in regression, the weights are learned by minimizing a cost function
\begin{align}
J(\bm{\hat{\beta}}) = \sum_{i=1}^{n} \big(\hat{y}_i - y_i \big)^2
\label{eq:cost}
\end{align}
where $\bm{\hat{\beta}}$ is a vector of estimated model parameters ($\hat{\beta}_0, \: \hat{\beta}_1, ..., \: \hat{\beta}_n$), $\hat{y}_i$ is a prediction, and $y_i$ is an observed value\cite{smlbook}. 
%In \cref{eq:cost} the squared error loss is used as loss function, and the cost is simply the loss averaged over the training data.\footnote{How cost and loss functions are defined can vary slightly in the literature, but in this work, the same definitions as in Lindholm et al. \cite{smlbook} are adopted.} 
Depending on the regression model, minimizing $J(\bm{\hat{\beta}})$ with respect to the model parameters is approached differently, as explained further in the sections below. 

\subsection{Linear regression models}

%\paragraph{Multiple linear regression}
From the wealth of available regression techniques, multiple linear regression (MLR) has been extensively used to forecast and model air pollution \cite{atmos7020015}. Generally, if none of the basic model assumptions are violated, MLR is a straightforward method. 
%especially for data with no temporal dependencies (so-called cross-sectional data). 
However, air pollution monitoring typically produces time series data, for which the assumption of independent errors is often not appropriate \cite{Montgomery2012}.
%is typically measured hourly or daily, and for this type of data, i.e., time series data, the assumption of independent errors is often not appropriate \cite{Montgomery2015}. 

\subsubsection{Linear regression for time series data}
If fitting a MLR model to time series data, successive errors will typically be correlated (often referred to as autocorrelation), and this will cause several problems with the model if the correlation is not accounted for \cite{Montgomery2015}. To this end, adjustments to the MLR model can be made, some of which will require other parameter estimation techniques than the standard method of ordinary least squares (OLS). However, a simple and commonly used procedure to eliminate the autocorrelation is to include one or more lagged values of the response variable as predictors. For example, if the value of the response variable at lag one ($y_{t-1}$) is included, the MLR model will have the form 
\begin{align}
y_t = \beta_0 + \beta_{1} y_{t-1} + \beta_2 x_{2,t} + ... + \beta_{k} x_{k,t} + \varepsilon_t, \: \: \: t = 1, 2, ..., T
\label{eqn:mlr}
\end{align}
where $\varepsilon_t$ is the the error term, and $t$ denotes time steps \cite{Montgomery2015}. The model in \cref{eqn:mlr} can be fit with OLS, which in linear regression is the standard way of finding model parameters so that $J(\bm{\hat{\beta}})$ is minimized \cite{Montgomery2012}. This is done by solving the so-called least squares normal equations,
%\begin{align}
%(\bm{X}^T\bm{X})\bm{\hat{\beta}} = \bm{X}^T\bm{y}
%\label{eq:normal_eq}
%\end{align}
and the least squares estimates of the model parameters are then given by \cref{eq:normal_sol} below, where $X$ is a matrix of regressor variables and $y$ is a vector of response variables. 
%(provided that the inverse of $\bm{X}^T\bm{X}$ exists) 
%are given by \cref{eq:normal_sol} below.
\begin{align}
\bm{\hat{\beta}} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}
\label{eq:normal_sol} 
\end{align}
A commonly used test for detecting autocorrelation is the Durbin-Watson test, where the statistic will have a value of $\sim 2$ in the case if uncorrelated errors \cite{Montgomery2015}. 
%where a value of the $\sim 2$ is expected in the case if uncorrelated errors \cite{Montgomery2015}.

\subsubsection{Robust linear regression models}
The errors of a MLR model should ideally be independent, have constant variance, and be approximately normally distributed \cite{Montgomery2012}. For inference and prediction, the normality assumption is important. Deviations from normality can sometimes be reasonably ignored, however, when the error distribution has long (or heavy) tails, this can be a sign of many extreme values in the data, in which case so-called robust estimation techniques are more appropriate than OLS \cite{Montgomery2012}. Typically for air pollution data, there are recurring periods where extreme values are frequent (for NO$_2$ this often happens during winter months, see \vref{fig:time_series_plots}), and for this reason, robust regression might be better suited than OLS regression for modeling and forecasting. 

One robust estimation technique is $M$-estimation, where a modified cost function is minimized to find the best parameter estimates:
%a modified version of \cref{eq:cost} is used to find the best parameter estimates:
\begin{align}
J(\bm{\hat{\beta}}) = \sum_{i=1}^{n} \rho \big(\hat{y}_i - y_i \big)^2.
\label{eq:robust_eq} 
\end{align}
In \cref{eq:robust_eq}, $\rho$ is a so-called robust criterion function, for which there are several alternatives, but a popular choice is the Huber's $t$ function (or Huber's method), where 
$$
\rho (z) =  \begin{cases}
\hfil \frac{1}{2}z^2, & \text{if} \:\: |z| \le t \\
|z|t - \frac{1}{2}t^2, & \text{if}\:\: |z| > t
\end{cases}
\label{eq:robust}
$$
and where $t$ is a robust estimate of $\sigma$ \cite{Faraway2020}. In general, minimizing \cref{eq:robust_eq} is done iteratively by solving weighted least squares normal equations and (at each iteration) recomputing the weights until convergence is reached \cite{Montgomery2012}. In effect, this makes the parameter estimation procedure much less sensitive to outliers and extreme observations in the data.

%Air pollution typically has periodicity where frequent extreme values are more common during certain seasons...

% the difference between regular

% air pollution data... extreme values ...depending on the traffic and weather conditions, peaks 

%that places less weight on the errors in the tails of the distribution

%\subsubsection{Ridge and lasso regression}
%Careful variable selection in MLR is crucial as it can influence the performance of a model, and one is often concerned with finding an optimal ”subset” of predictors, where multicollinearity should also not be an issue \cite{Montgomery2012}. To this end, variable selection techniques based on optimizing a criterion like the Akaike or Bayes information criterion are common, and typically multicollinearity is also tested for (e.g.\ by examining the condition number, where a value below 100 is preferable) \cite{Montgomery2012}. 
%
%However, if one is reluctant to exclude variables, but multicollinearity still might be an issue, regularized versions of MLR can be used \cite{Montgomery2012}. Two common techniques are $L_1$ and  $L_2$ regularization, in which an extra so-called "penalty" term is added to the cost function to shrink (and essentially stabilize) the model parameter estimates \cite{smlbook}. In $L_2$ regularization (also called ridge regression), the parameters will be pushed towards small values, whereas in $L_1$ regularization (or lasso regression), some parameters will be driven to zero. 
%The penalty terms for ridge and lasso regression are, respectively, 
%$$\lambda \sum_{j=1}^{k}\beta_{j}^2\:\:\: \text{and}\:\:\: \lambda \sum_{j=1}^{k}|\beta_{j}|$$
%where $\lambda$ is a parameter controlling the shrinkage \cite{smlbook}. For ridge regression, the parameter estimates can be found by solving a modified version of \vref{eq:normal_sol}, while for lasso regression, no such analytical solution exists, and numerical optimization techniques have to be utilized \cite{smlbook}. 
%Generally, $L_1$ and $L_2$ regularization can be used to prevent overfitting, and also for $L_1$ where some input variables can be eliminated, as a variable selection technique \cite{smlbook}. 

\subsubsection{Additional considerations for linear models}
%As pointed out in \cref{sec:airpollutants}, weather conditions can affect pollution levels, which is why meteorological parameters often is included as input variables \cite{atmos7020015}. 
Careful variable selection in MLR is also crucial as it can influence the performance of a model, and one is often concerned with finding an optimal ”subset” of predictors, where multicollinearity should also not be an issue \cite{Montgomery2012}. To this end, variable selection techniques based on optimizing a quantity of interest, e.g.\ the Akaike information criterion or the root mean squared error (RMSE) are common, and a diagnostic for multicollinearity is the condition number, where a value below 100 does not indicate any serious problems \cite{Montgomery2012}. 
%With many input variables, it is common to use regularized versions of MLR such as ridge or lasso regression, in which an extra so-called "penalty" term is added to the cost function to shrink (and essentially stabilize) the model parameter estimates \cite{Montgomery2012, smlbook}.

The extensive use of MLR for air pollution forecasts is many times motivated by its simplicity and straightforward implementation \cite{atmos7020015}. Another advantage is interpretability; for example, inference can be made on all input variables, allowing one to investigate their individual importance and relationship to the response variable \cite{Montgomery2012}. However, the statistical properties of MLR make it rather restrictive as a model, and not all violations of the assumptions can be remedied (like non-linearity) \cite{Montgomery2015}. More flexible regression models, better suited to capture complex input-output relationships, have also found extensive use in air pollution forecasts \cite{atmos7020015}, and in the next section, some of these models are reviewed. 

%and large forecast errors have been observed at times of pollution peaks \cite{atmos7020015}. Moreover, with data from several (but nearby) monitoring stations, multicollinearity among the input variables can be an issue, which is why ridge or lasso regression are common alternatives to the non-regularized MLR model \cite{atmos7020015}. \textcolor{red}{ARIMA models???}

%Linear models for time-series analysis, such as auto-regressive moving average and auto-regressive \textit{integrated} moving average (ARMA and ARIMA, respectively) and variants thereof, are also common \cite{Arsov2021, Goyal2006}. These models make predictions of future values based on past data (i.e., taking dependencies over time into account), however, similar to MLR, linearity is assumed and errors can be large when there are temporary peaks in pollution levels \cite{atmos7020015}. 

%\paragraph{Bayesian methods}
%While the forecasting techniques discussed above produce point-predictions of a pollutant, Bayesian methods can be used to predict distributions (or put another way, make density forecasts) \cite{FaganeliPucer2018}. A Bayesian approach can offer some advantages if thresholds are of interest, since with a density forecast, the probability of pollution levels exceeding a certain value can be estimated \cite{FaganeliPucer2018, smlbook}. For example, in Pucer et al. \cite{FaganeliPucer2018}, a Gaussian process model was used to give Gaussian density predictions of PM$_{10}$ and O$_3$. Bayesian methods have also been used for spatial predictions of PM by spatial interpolation (using values from monitored locations to estimate levels at other locations without any monitoring) \cite{atmos7020015}. 

\subsection{More complex regression models and neural networks}
\label{sec:deep_learning}
For air pollution, more flexible regression techniques tend to give better forecasting results than the more simplistic linear models \cite{atmos7020015}. Some examples include regression trees, support vector regression (SVR), and artificial neural networks \cite{atmos7020015, FaganeliPucer2018}. In many ways, these regression techniques are extensions to the linear model, e.g., SVR is built on the idea of constructing non-linear transformations of the regressors, whereas the building blocks of artificial neural networks are what is often called (artificial) neurons, where the output of a linear regression model is transformed through a non-linear activation function \cite{smlbook}. In this work, a number of neural network architectures are utilized, and a deeper dive into the theory behind them is warranted. 

\subsubsection{The artificial neural network model}
As described above, in an artificial neuron, the output of a linear regression model is passed through a non-linear activation function; some choices for activation functions include the rectified linear unit (ReLU), or the tanh function:

\begin{gather}
    \text{ReLU}(x) = \text{max}(0,x) \label{eq:relu} \\[1ex]
    \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \label{eq:tanh}
\end{gather}
\noindent
A common way to illustrate an artificial neuron is shown in \cref{fig:perceptron}, where the inputs (including the so-called bias term, which is always equal to 1), are multiplied by the corresponding weights (w), and where the linear combination (z) subsequently gets transformed by the activation function $f$ in the node before an output is produced.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=1.15]{perceptron}
\caption{An artificial neuron illustrated.}
\label{fig:perceptron}
\end{center}
\end{figure}

A layer in a neural network is made up of several nodes (also called hidden units), and in a deep neural network, many such layers can be stacked sequentially, so that the outputs of the nodes in one layer becomes the input to the nodes in the next \cite{smlbook}. Artificial neural networks can have many thousands of trainable parameters/weights, which makes them extremely flexible. However, this comes at a great computational cost, and neural networks are also very prone to overfitting, which is why regularization techniques frequently have to be used \cite{smlbook}. A common regularization technique is dropout, which is explained further below. 

\subsubsection{Deep learning architectures}
\label{sec:DL models}
The most basic type of neural network model is the fully connected neural network (fcNN). (This is also sometimes called a densely connected neural network, or simply, a dense model). A fcNN basically has the structure of the neural network model described above, i.e., with nodes in layers stacked sequentially, and where at each layer, a slightly more abstract representation of the inputs are produced by non-linear transformations. With several such transformations, complex relationships in the data can often be learned \cite{LeCun2015, smlbook}.

%Depending on the problem, other types of specialized neural network models can be used. For example, convolutional neural networks (CNNs) are common in image processing tasks, 

For data where order matters (like with sequence and time series data), so-called recurrent neural network (RNN) models are often more appropriate than the fcNN \cite{LeCun2015}. RNN's can utilize past information in a sequence through a so-called 'state vector', which works like a memory, holding the information from previous time steps. The state vector is also continuously updated as a data sequence is being processed, so the output at any time $t$ depends on the current input as well as the information contained in the "memory" (or the systems hidden state) \cite{LeCun2015}. The most basic RNN architecture, the simple RNN model, can be difficult to train, however, and typically also performs poorly when processing longer sequences due to the way the state vector is maintained and updated \cite{LeCun2015}. For this reason, a much more common RNN model is the so-called long short-term memory (LSTM) model. The LSTM model utilizes three so-called gates (the input gate, the output gate, and the forget gate), which together with an additional vector (the cell-state vector), controls the information flow through the units of the network \cite{LeCun2015}. Yet another common RNN model is the gated recurrent unit (GRU) model, which is similar to the LSTM model but with fewer parameters \cite{Chollet2017}. 

\subsubsection{Training and tuning artificial neural networks}
To train the parameters of a neural network, initial random values for the parameters are set, and the value of the cost function along with its vector of gradients are calculated (the backpropagation algorithm is typically used for this). 

%the backpropagation algorithm is used to calculate the value of the cost function along with its vector of gradients. These are then used in 

% initialize values, 
% compute cost function with respect to all the parameters, 
% compute gradient, 
% adjust the parameters according to 

%To train the many parameters of a neural network, the overall cost (i.e., the average of \cref{eq:cost} taken over the training data) is minimized \cite{smlbook, Montgomery2015}. Typically, this is done with the backpropagation algorithm, in which random values are initially assigned the parameters, 

%have in their hidden units a so-called 'state vector', which in a way works like a memory as it contains past information. When a sequence is being processed by a RNN, the state vector is continuously updated, and in this way the output at any time step $t$ depends on the current input as well as the information contained in the state vector (the systems "hidden state") \cite{LeCun2015}. 


%When a set of input data is taken and passed through the layers of a neural network, each layer produces a slightly more abstract representation of its input by non-linear transformations, and with several such transformations, complex relationships in the data can be learned \cite{LeCun2015, smlbook}. 


%A collection of connected artificial neurons is what constitutes a layer in a neural network, and a network with many such layers stacked sequentially is what is referred to as a deep neural network \cite{smlbook}. 

%Common activation functions include the sigmoid function, the rectified linear unit (ReLU) function, and the Tanh functions:

%As indicated above, artificial neural networks are made up of linear regression models stacked sequentially, where the output of each such layer is 

%is the building block of neural networks \cite{smlbook}. Often, several layers of such regression models are stacked sequentially, resulting in what is called a deep neural network. 

%output modified
 
\subsection{Evaluating regression models and forecasting performance}
\label{sec:reg_metrics}
\subsubsection{Common regression metrics}
When evaluating the performance of a forecasting model, one is often concerned with the one-step ahead forecast error, defined as
$$e_t(1) = y_t - \hat{y}_t (t-1)$$ 
where $\hat{y}_t (t-1)$ is the forecasted value of $y_t$ made at time $t-1$, and accuracy metrics such as the mean error (ME), the mean absolute error (MAE), the mean squared error (MSE), and the mean absolute percent forecast error (MAPE) are often utilized \cite{Montgomery2015}. These performance metrics are standard when evaluating regression models, and the equations for them are not repeated here. It can be noted though that the ME is an estimate of the expected value of $e_t(1)$, and strong departures from zero would indicate bias in the forecasts \cite{Montgomery2015}. Also, commonly the root mean squared error (RMSE) is used in connection with the MSE, as the MSE gives the magnitude of the errors in squared, and not original, units. The MSE is an estimate of the variance of $e_t(1)$, and consequently the RMSE becomes an estimate of the standard deviation \cite{Montgomery2012}. 

\subsubsection{Hypothesis testing and inference}
Hypothesis tests can also be carried out for the regression metrics, where one is concerned with whether the differences between competing models hold or are simply due to chance. If the same dataset is used for many models, a suitable test is the one-way analysis of variance (ANOVA) test, in which the means of independent groups under several treatments are compared \cite{Wackerly2007}. One could e.g.\ then test whether there is a significant difference in the MSE between the models. The ANOVA test requires that the samples come from a normal distribution, however, a non-parametric alternative is the Kruskal-Wallis test \cite{Wackerly2007}. Following the ANOVA or the Kruskal-Wallis test, pairwise comparisons between groups can be carried out with either Tukey's test or the Dunn's test (which are, respectively, a parametric and a non-parametric test).  

\subsubsection{Tests for autocorrelation in forecast errors}
\label{sec:auto_errors}
An important concept in time series analysis is that of autocovariance and autocorrelation \cite{Montgomery2015}. The autocovariance and  the autocorrelation coefficient for two observations $k$ lags apart in a time series are given, respectively, by \cref{eq:covariance} and \cref{eq:correlation} below.  

\begin{gather}
     \upgamma_k = \text{Cov}(y_t, y_{t+k}) = E[(y_t - \mu)(y_{t+k} - \mu)] \label{eq:covariance} \\[1ex]
     \rho_k = \frac{E[(y_t - \mu)(y_{t+k} - \mu)]}{\sqrt{E[(y_t - \mu)^2]E[(y_{t+k} - \mu)^2]}} = \frac{\text{Cov}(y_t, y_{t+k})}{\text{Var}(y_t)} = \frac{\upgamma_k}{\upgamma_0}\label{eq:correlation}
\end{gather}
The autocovariance function is the sequence of values of $\upgamma_k$ for $k=0,1,2 ..., T$, and similarly, the autocorrelation function (ACF) is the sequence of values of $\rho_k$ for $k=0,1,2 ..., T$. In practice, these functions can only be estimated, and the estimates for the autocovariance function and the ACF (called the sample ACF) are given by \cref{eq:sample_covariance} and \cref{eq:sample_correlation}, respectively.

\begin{gather}
     c_k=\hat{\upgamma}_k = \frac{1}{T} \sum_{t=1}^{T-k} (y_t - \bar{y})(y_{t+k} - \bar{y}), \:\:\:\: k = 0,1,2,...,K \label{eq:sample_covariance} \\[1ex]
     r_k = \hat{\rho}_k = \frac{c_k}{c_0}, \:\:\:\: k = 0,1,2,...,K \label{eq:sample_correlation}
\end{gather}

In general, a time series is said to be white noise if the observations are uncorrelated and have constant variance. If the observations are also normally distributed, the time series is said to be Gaussian white noise, and ideally, this should be the case for the forecast errors \cite{Montgomery2015}. If the forecast errors are white noise, the sample ACF will approximately follow a normal distribution with zero mean and variance $1/T$. To test whether any of a set of autocorrelations in the sample ACF are not zero (which then would indicate that the forecast errors are not white noise) the Box-Pierce test can be used \cite{Montgomery2015}. Essentially, this is a goodness-of-fit test where it is determined how well the sample ACF fits to the ACF of white noise. A similar and commonly used test for smaller sample sizes is the Ljung-Box test \cite{Montgomery2015}.

\subsubsection{Additional evaluation metrics}
There are many additional ways in which the performance of a forecasting model can be evaluated. For example, in a study from 2018 by Pucer et al. \cite{FaganeliPucer2018}, predictions are classified and cost matrices are utilized as part of model evaluation, and in a paper from 2007 by Stablober et al. \cite{Stadlober2008}, a quality function is defined in which each observation--forcast pair are rated, and where a larger penalty is also put on faulty forecasts in a specific range. In this work however, the common regression metrics are used together with a close examination of the forecast errors (using the methods described above) in order to answer the main research question of the thesis.  

%Therefore, the hypothesis that the autocorrelation is zero at each lag can be tested for every $k$ up to a desired timestep $T$. However, a more convenient approach is to test whether a set of autocorrelations are zero, thereby indicating if the time series is white noise.  


%have an approximate normal distribution with zero mean and variance $1/T$ \cite{Montgomery2015}. 
%
%normal with zero mean and variance $1/T$. 

%Similarly as the errors pertaining to model fitting in linear regression, also the forecast errors should ideally be uncorrelated, normally distributed, and have constant variance (i.e., the forecast errors should be Gaussian white noise) \cite{Montgomery2015}. If the errors 

%Besides the common regression metrics described in the section above, 
%If the linearity assumption does not hold, a linear model is not an appropriate choice.  

%More versatile and flexible regression models tend to give better forecasting results than linear models \cite{atmos7020015}. Some commonly used models include regression trees, generalized additive models, and support vector machines (SVM) \cite{atmos7020015, FaganeliPucer2018}. These models can handle more complex non-linear input-output relationships, and especially SVM has been successfully applied for PM$_{10}$ prediction, sometimes with better results than artificial neural networks \cite{atmos7020015}. 
%
%Artificial neural networks (ANNs), in particular the multilayer perceptron (MLP), have also been extensively used as a forecasting technique \cite{atmos7020015}. ANNs are flexible models able to handle non-linear input-output relationships, however, over-fitting can be an issue, especially with high-dimensional input and if training data is limited \cite{atmos7020015, FaganeliPucer2018}. 
%
%%The MLP is a so-called feedforward neural network, in which a set of inputs are taken, passed through several layers of so-called hidden units, eventually producing an output \cite{LeCun2015}. 
%The MLP is a so-called feedforward neural network, in which a set of input data is taken and passed through several "hidden" layers made up of neurons (also called units), before an output is produced \cite{LeCun2015}.
%%A common way to illustrate a MLP is given in Figure \ref{fig:ANN}, where a network consisting of the input layer, two hidden layers (where units are represented with circles), and a single output, is shown. 
%Deep neural networks can have many such layers (hence the term "deep" \cite{Chollet2017}), and each layer can have hundreds of units. Every layer produces a slightly more abstract representation of its input by non-linear transformations, and with several such transformations, complex relationships in the data can be learned \cite{LeCun2015}.

%\begin{figure}[h]
%\begin{center}
%\includegraphics{neural-network}
%\caption{Artificial neural network with two hidden layers.}
%\label{fig:ANN}
%\end{center}
%\end{figure}

%Training the neural network is an iterative process, in which the weights (or parameters) of the network are adjusted until the measured error stops decreasing.

%Many other deep learning architectures than the MLP exist, such as convolutional neural networks (CNNs), or recurrent neural networks (RNNs). CNNs are commonly used for image recognition while RNNs (and variants thereof) normally are applied to sequential data. 
%\textcolor{red}{(This section will be expanded with some more theory for the deep learning models to be used. Also, some mathematical notation will added.)}

%\subsection{Variable selection}
%
%In any regression problem, variable/feature selection is crucial as it can influence the performance of a model. 
%%, and rarely are all available input features necessary or even desirable to include (as some might worsen performance) \cite{smlbook}. 
%%As pointed out in section \ref{sec:airpollutants}, 
%With regards to PM, as pointed out in section \ref{sec:airpollutants}, weather conditions can greatly affect pollution levels, and therefore meteorological data can be utilized to improve forecasts \cite{atmos7020015}.
%
%% ### CORRELATION PLOT ###
%\begin{figure}[h]
%\makebox[\textwidth][c]{\includegraphics[width=0.85\textwidth]{/Users/simoncarlen/desktop/luftdata/plots/Torkel Knutssongatan_correlation}}
%\caption{Pairwise correlations between air pollutants and some meteorological variables.}
%\label{fig:correlationplot}
%\end{figure}
%% #######################
%
%Additional variables can also be included in PM forecasts. For example, motor traffic data such as travel speeds, traffic flow and intensity, etc., can be utilized \cite{atmos7020015}. Data on other pollutants can also be important, especially SO$_2$ and NO$_x$ as they are involved in the formation of secondary PM \cite{Arsov2021}. Moreover, if forecasts focus solely on PM$_{10}$ (as in this work), data on PM$_{2.5}$ can further improve the results  \cite{Arsov2021}. Temporal variables such as time of the day and time of year are also useful since daily and seasonal variation of PM pollution is important \cite{Schwarzenbach2016, atmos7020015}. 
%%The selection and preprocessing of variables in this work is described in detail in section \ref{chap:dataprocesschap}. 
%%A detailed description of all variables utilized in this work (and how they were processed), is given in section \ref{chap:dataprocesschap}.
%
%In Figure \ref{fig:correlationplot} where pairwise correlations between a few meteorological variables and PM$_{10}$ at different stations in the Stockholm region are given (see Table \ref{tab:stations} for details about the different monitoring stations), it can be inferred that PM$_{10}$ correlate negatively with humidity, but positively with atmospheric pressure and solar radiation. It can also be seen that PM$_{10}$ levels are strongly correlated among some stations. 
%
%In this work, in addition to PM$_{10}$ data, meteorological data as well as data on PM$_{2.5}$ and NO$_x$ were utilized. Some features used as input to the models were also derived. A more detailed description of the variables and their preprocessing is given in section \ref{chap:dataprocesschap}.

%\section{Summary and motivation for this work}