%General implementation of the stategy here before going into details about sources, preprocessing, hyperparameter tuning etc, ... Perhaps also a pic?
The major steps of the implemented workflow is shown in Figure \ref{fig:dataflow}. Historical air pollution data from several monitoring stations, together with meteorological data from one station, was retrieved, preprocessed (with some features engineered), and divided into data windows. Three deep learning models (feed forward neural network, RNN, and LSTM) were trained and tested for short-term predictions (one hour ahead) of PM$_{10}$ for one station at Torkel Knutssongatan (measuring urban background levels, see Table \ref{tab:stations}). As baseline models for comparison, multiple linear regression and ARIMA were used. Detailed descriptions of each step in the process are given in subsequent sections. 

\begin{figure}[htbp]
\begin{center}
\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{workflow}}
\caption{Implemented workflow.}
\label{fig:dataflow}
\end{center}
\end{figure}

\section{Data retrieval and preprocessing}
\label{chap:dataprocesschap}

\subsection{Data sources}
\label{sec:data-sources}

Air pollution data was retrieved from the Swedish Meteorological and Hydrological Institute's (SMHI) centralized database for air quality measurements \cite{smhi-luftmatningar}. This data is part of the national and regional environmental monitoring of Sweden, a program coordinated and funded by the Swedish Environmental Protection Agency (Swedish EPA) and the Swedish Agency for Marine and Water Management. There are in total ten different program areas, of which air is one, and all data are licensed under CC0 and therefore freely accessible to the public \cite{naturvardsverket-miljodata}. For the national air monitoring (under Swedish EPA's responsibility), SMHI acts as national data host and stores (quality checked) historical data reported on a yearly basis from municipalities in Sweden \cite{smhi-luftmatningar}.

In Stockholm County, there are 18 stationary monitoring sites, and initially, data from each station was considered. However, many stations had irregular data series, and not all of them measure the same set of parameters. Due to this, data from seven sites with hourly measurements of PM$_{10}$ and PM$_{2.5}$ (\textmugreek g/m$^3$) for the period 2016-01-01 (01:00 am) to 2020-09-16 (01:00 am) was chosen, giving in total 41,281 data points (per parameter). In six of these stations, hourly data for NO$_x$ was also available. Air pollution monitoring can be classified by the area surrounding the station (rural, rural-regional, rural-remote, suburban, and urban), and by the predominant emission sources (background, industrial, or traffic) \cite{smhi-luftmatningar}. The chosen stations included data from both traffic and background monitoring, in urban as well as rural-regional areas. Information about the stations from which data was used is summarized in Table \ref{tab:stations}. 

As described in section \ref{air-pollution-stockholm}, SLB-analys also monitor a number of weather parameters, and hourly measurements of temperature (in $^\circ$C), precipitation (mm), atmospheric pressure (hPa), relative humidity (as \%), solar radiation (W/m$^2$), wind speed (m/s), and wind direction (in degrees) was included from the station at which the PM$_{10}$ predictions were made (Torkel Knutssongatan). The meteorological data was downloaded from SLB-analys' webpage \cite{slb-analys-meteorologi}. 

% STATIONS TABLE
% ##############

\begin{table}[h]
\caption{Summary of monitoring stations in Stockholm County.}
\label{tab:stations}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
Station                         & Station code & Longitude & Latitude  & Classification & Parameters                    \\ \midrule
Norrtälje, Norr Malma &
  18643 &
  18.631313 &
  59.832382 &
  \begin{tabular}[c]{@{}l@{}}Rural-regional \\ background\end{tabular} &
  PM$_{10}$, PM$_{2.5}$, NO$_x$ \\ \midrule
Sollentuna, E4 Eriksbergsskolan  & 34399        & 17.957651 & 59.410175 & Urban background   & PM$_{10}$, PM$_{2.5}$         \\ \midrule
Sollentuna, E4 Häggvik           & 20415        & 17.922358 & 59.443535 & Urban traffic      & PM$_{10}$, PM$_{2.5}$, NO$_x$ \\ \midrule
Stockholm, E4/E20 Lilla Essingen & 18644        & 18.00439  & 59.325527 & Urban traffic      & PM$_{10}$, PM$_{2.5}$, NO$_x$ \\ \midrule
Stockholm, Hornsgatan 108 Gata   & 8780         & 18.04866  & 59.317223 & Urban traffic      & PM$_{10}$, PM$_{2.5}$, NO$_x$ \\ \midrule
Stockholm, Sveavägen 59 Gata     & 8779         & 8.058254  & 59.340828 & Urban traffic      & PM$_{10}$, PM$_{2.5}$, NO$_x$ \\ \midrule
Stockholm, Torkel Knutssongatan &
  8781 &
  18.057808 &
  59.316006 &
  Urban background &
  \begin{tabular}[c]{@{}l@{}}PM$_{10}$, PM$_{2.5}$, NO$_x$,\\ Meteorological \\ parameters\end{tabular} \\ \bottomrule
\end{tabular}%
}
\end{table}

% ##############

\subsection{Data preprocessing}

Time series plots of the raw data for PM$_{10}$ and PM$_{2.5}$ is shown in Figure \ref{fig:time_series_plots}. 
%(station-wise, with PM$_{10}$ and PM$_{2.5}$ values in the left and right subplots, respectively). 
As can be seen e.g. in plot (e) and (f), some stations had short periods with missing data, and linear interpolation was used to fill in the missing data points. Moreover, all data was min-max normalized (i.e., scaled to the range [0,1]) before use in any of the models. 

\paragraph{Feature engineering}
From the meteorological data, wind vectors ($u$ and $v$) were derived from wind direction and wind speed, as wind vectors are more suitable model inputs \cite{tensorflow-timeseries}. After converting wind direction values to radians, $u$ and $v$ were obtained in the following way $$ u = ws * cos(\theta)$$ $$v = ws * sin(\theta)$$ where $ws$ denote wind speed and $\theta$ is the wind direction (in radians). From Figure \ref{fig:timeseriesPM10}, yearly periodicity in the data can be seen, where levels tend to be higher during spring. Daily periodicity is also expected since traffic intensity varies throughout the day. The meteorological parameters such as temperature, solar radiation, etc.\ also have yearly and daily periodicity. To account for this, time was converted to sine and cosine signals, both for year and day, as a way to model these cyclic features \cite{FaganeliPucer2018}. Sine and cosine for day were calculated in the following way

$$ sine \: day = \frac{sin \Big (timestamp\: \cdot \frac{2\pi}{86,400} \Big)+1}{2} $$

$$ cosine \: day = \frac{cos \Big (timestamp\: \cdot \frac{2\pi}{86,400} \Big)+1}{2} $$
where $timestamp$ is in seconds (and with 86,400 seconds in 24 hours, dividing by this term is necessary). The calculations were done similarly for year except for the term in the denominator which instead was set to seconds per year ($365.25 \cdot 86,400 $). The transformations were done so that the sine and cosine functions oscillate between zero and one. The resulting transforms for day (in a 24 hour time window) and year (time window of one year) are shown in Figure \ref{fig:time_sine_cos}a and Figure \ref{fig:time_sine_cos}b, respectively.


\begin{figure}[h] 
\begin{center}
%\includegraphics{/Users/simoncarlen/desktop/thesis/data_and_code/plots/time_of_day_year_signal}
\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{/Users/simoncarlen/desktop/luftdata/plots/time_signals}}
\caption{(a) Sine and cosine of day, and (b) sine and cosine of year.}
\label{fig:time_sine_cos}
\end{center}
\end{figure}

\paragraph{Sliding windows}
Sliding windows from the data were also created. The sliding window approach is used for time-series forecasting where windows (or sequences of certain lengths, also called frames) are extracted from the input data \cite{Arsov2021, Gilik2021}. In each window, there are two "sub-windows"; the input window and the target window, and the target window is offset by some amount of time from the input window. For example, as shown in Figure \ref{fig:sliding-window}, the total window length is nine time steps, and the first eight time steps is the input window used to predict the target window (in this case having a length of one) one time step in the future. After extracting a data sequence, the window slides to the right one (or more) steps and extracts the next sequence. This is continued until time step $n$ at which point all the data have been processed. 
In this work, input windows of different lengths were tested to make short-term predictions for a target window with a length of one (more details are given in section \ref{sec:tuning} below). 

 \begin{figure}[h]
\begin{center}
\includegraphics{sliding-windows}
\caption{Sliding window approach for time-series data.}
\label{fig:sliding-window}
\end{center}
\end{figure}

%In this work, input window lengths were set to 12 hr, and predictions were made 1 hr, 6 hr, and 12 hr in the future (giving total window lengths of 13 hr, 18 hr, and 24 hr, respectively). Moreover, single-output models predicting PM levels at one urban background station (Torkel Knutssongatan),  as well as multi-output models predicting PM levels at all urban traffic stations, were tested.

%In this work, the window lengths were set to 12 hours, and both single time-step predictions (target windows of length one) and multi-time-step predictions (target windows of length $n$) were tested. Moreover, prediction models giving both single-outputs (the target value at one monitoring station) as well as multi-outputs (target value at all stations) were constructed.

%\paragraph{Train-test split} Lastly, the data was split into training (60\%), validation (20\%), and test (20\%) sets, where the validation set was used for hyperparameter tuning (described in more detail in section \ref{sec:tuning}). Being sequence data, the sampling was done consecutively, without random shuffling, so that order information would be preserved \cite{Gilik2021}. 
\paragraph{Train-test split} Lastly, the data was split into training, validation, and test sets, where the validation set was used for hyperparameter optimization. The test set was taken as the most recent year of data (from 2019-09-16 to 2020-09-16), the validation set was taken as the year prior to the test data (2018-09-16 to 2019-09-16), and the remaining data was used for training (2016-01-01 to 2018-09-16). This split is motivated by the fact that the data is in the form of time-series, where each observation has a specific time-stamp and where successive observations are (in this case) positively autocorrelated \cite{Brockwell2016}.

%Being sequence data, the sampling was done consecutively, without random shuffling, so that order information would be preserved 

%/Users/simoncarlen/desktop/thesis/data_and_code/plots/PM10_2016_2020_2

\begin{figure}[h]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{../plots/time_series_plots}}
\caption{Time series plots for PM$_{10}$ and PM$_{2.5}$.}
\label{fig:time_series_plots}
\end{figure}

\section{Hyperparameter tuning and search}
\label{sec:tuning}

The Keras Tuner library \cite{omalley2019kerastuner} was used to find the best set of hyperparameters for each model (except for the MLR and ARIMA models used as baseline). 
%Hyperparameters at both the model architecture-level as well as the input data-level were included in the search space. More specifically, at the input data-level, the width of the windows were set to 3 h, 6 h, or 12 h, and the models were fit with the different versions of the input data. 
More specifically, the following hyperparameters were tuned:

\begin{itemize}
\item Number of layers (up to five were tested)
\item Number of units per layer (in the range [32, 512] with step size set to 32)
\item Learning rate (sampled uniformly in the range [0.0001, 0.01])
\item Number of epochs 
\end{itemize} 

%For some models, a dropout layer (with rates in the range [0, 0.3] and step size set to 0.05) was also tested.
For the hyperparameter search, Bayesian optimization was used as tuner. (The Bayesian optimization tuner tries to predict which hyperparameters that are likely to improve the model given previous results \cite{omalley2019kerastuner}). The motivation for this choice is the large number of possible hyperparameter combinations, making it infeasible to test all of them within a reasonable amount of time. Instead, it was assumed that the tuner after 75 trials would find some optimal set of hyperparameters. 
The hyperparameter search was done in total three times for every model; one search each was performed for data input windows of different sizes, namely 8 h, 16 h, and 24 h. After completing the search, the number of epochs for each model were tuned, and all models were re-trained and evaluated on the validation and test data. 


%, the models were re-trained on the training set plus the validation set, and performance on the test set was recorded. Again, with three different window sizes tested, three versions per model were obstained. All predictions were made for PM$_{10}$ one hour ahead for the station at Torkel Knutssongatan (measuring urban background levels in the center of Stockholm).
