%General implementation of the stategy here before going into details about sources, preprocessing, hyperparameter tuning etc, ... Perhaps also a pic?
The major steps of the implemented workflow were as follows; 

Detailed descriptions of each step in the process are given in subsequent sections

%is shown in Figure \ref{fig:dataflow}. Historical air pollution data from several monitoring stations, together with meteorological data from one station, was retrieved, preprocessed (with some features engineered), and divided into data windows. Three deep learning models (feed forward neural network, RNN, and LSTM) were trained and tested for short-term predictions (one hour ahead) of PM$_{10}$ for one station at Torkel Knutssongatan (measuring urban background levels, see Table \ref{tab:stations}). As baseline models for comparison, multiple linear regression and ARIMA were used. Detailed descriptions of each step in the process are given in subsequent sections. 

%\begin{figure}[htbp]
%\begin{center}
%\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{workflow}}
%\caption{Implemented workflow.}
%\label{fig:dataflow}
%\end{center}
%\end{figure}

\section{Data retrieval and preprocessing}
\label{chap:dataprocesschap}

\subsection{Data sources}
\label{sec:data-sources}

Air pollution data was retrieved from the Swedish Meteorological and Hydrological Institute's (SMHI) centralized database for air quality measurements \cite{smhi-luftmatningar}. This data is part of the national and regional environmental monitoring of Sweden, a program coordinated and funded by the Swedish Environmental Protection Agency (Swedish EPA) and the Swedish Agency for Marine and Water Management. There are in total ten different program areas, of which air is one, and all data are licensed under CC0 and therefore freely accessible to the public \cite{naturvardsverket-miljodata}. For the national air monitoring (under Swedish EPA's responsibility), SMHI acts as a national data host and stores (quality checked) historical data reported yearly from municipalities in Sweden \cite{smhi-luftmatningar}.

\subsubsection{Monitoring stations}
In Stockholm County, there are 19 stationary sites for air pollution monitoring \cite{slb-matningar}, and initially, data from each site was considered. However, many stations have irregular data series, and not all stations measure the same set of pollutants. Due to this, data from three sites with hourly measurements of PM$_{10}$ and PM$_{2.5}$ (in \textmugreek g/m$^3$) for the time period 2016-01-01 to 2022-01-01 was chosen, giving a total of 52,609 data points. For the station at which PM predictions subsequently were to be made (Torkel Knutssonsgatan), hourly data of NO$_2$ was also included. As described in section \ref{air-pollution-stockholm}, SLB-analys also monitor several weather parameters, and hourly measurements of temperature (in $^\circ$C), precipitation (mm), atmospheric pressure (hPa), relative humidity (as \%), solar radiation (W/m$^2$), and wind speed (m/s) were also included from the station at Torkel Knutssonsgatan. The meteorological data were downloaded from SLB-analys' webpage \cite{slb-analys-meteorologi}. In general, air pollution monitoring can be classified by the surrounding area (rural, rural-regional, rural-remote, suburban, and urban), and by the predominant emission sources (background, industrial, or traffic) \cite{smhi-luftmatningar}. The chosen stations included data from both traffic and background monitoring, in urban as well as rural-regional areas. More information about the stations are given in Table \ref{tab:stations} in appendix \ref{chapt:appendix_A}. 

%% STATIONS TABLE
%% ##############
%\begin{table}[]
%\centering
%\caption{Monitoring stations in Stockholm County.}
%\label{tab:stations}
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{@{}llllll@{}}
%\toprule
%Station                         & Station code & Longitude & Latitude  & Type of monitoring                                                   & Parameters                                                                                             \\ \midrule
%NorrtÃ¤lje, Norr Malma           & 18643        & 18.631313 & 59.832382 & \begin{tabular}[c]{@{}l@{}}Rural-Regional \\ Background\end{tabular} & PM$_{10}$, PM$_{2.5}$                                                                                  \\ \midrule
%Stockholm, Hornsgatan 108       & 8780         & 18.04866  & 59.317223 & Urban Traffic                                                        & PM$_{10}$, PM$_{2.5}$                                                                                  \\ \midrule
%Stockholm, Torkel Knutssonsgatan & 8781         & 18.057808 & 59.316006 & Urban background                                                     & \begin{tabular}[c]{@{}l@{}}PM$_{10}$, PM$_{2.5}$, NO$_2$, \\ meteorological \\ parameters\end{tabular} \\ \bottomrule
%\end{tabular}%
%}
%\end{table}
%% ##############

\subsection{Data preprocessing}
\subsubsection{Initial preprocessig}
Time series plots for PM$_{10}$ and PM$_{2.5}$ at Torkel Knutssonsgatan are shown in \vref{fig:time_series_plots}. (Similar plots but for all stations are given in Fig.\ \ref{fig:time_series_plots_all} in appendix \ref{chapt:appendix_A}.)
%(station-wise, with PM$_{10}$ and PM$_{2.5}$ in the left and right subplots, respectively)
Some stations had short episodes with missing data, and linear interpolation was used to fill in the missing values. However, for the PM$_{2.5}$ data at Torkel Knutssonsgatan (plot (b) in \vref{fig:time_series_plots}), due to the rather big gap at the beginning of 2019, a train-test split was done to entirely avoid this period (see below). Missing weather data was also linearly interpolated, except for the variables atmospheric pressure and wind speed for which mean imputation was deemed more appropriate. 
%Moreover, before use in any of the models, all data were min-max normalized (i.e., scaled to the interval $[0,1]$). 

It should be noted that some PM values were negative.
%this is seen clearly in e.g.\ plot (f) in \vref{fig:time_series_plots}, for the time period Jan.\ 2016 up to about Sep.\ 2019. 
However, negative values are expected since automated measuring instruments for PM (due to noise) may produce values between zero and the negative detection limit, especially when there are rapid changes in humidity (SMHI, personal communication, April 11, 2022). These values are therefore not to be considered any more "incorrect" than positive values, though it may at first seem contradictory to include them in an analysis. 

\begin{figure}[h]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{../plots/time_series_plots_target_station}}
%\includegraphics[width=\textwidth]{../plots/time_series_plots}
\caption{Time series plots for (a) PM$_{10}$ and (b) PM$_{2.5}$ at Torkel Knutssonsgatan.}
\label{fig:time_series_plots}
\end{figure}

%\begin{figure}[h]
%\centering
%\makebox[\textwidth][c]{\includegraphics[width=0.9\textwidth]{../plots/time_series_plots_7}}
%\caption{Time series plots for PM$_{10}$ and PM$_{2.5}$.}
%\label{fig:time_series_plots}
%\end{figure}

%\paragraph{Feature engineering}
%From the meteorological data, wind vectors ($u$ and $v$) were derived from wind direction and wind speed, as wind vectors are more suitable model inputs \cite{tensorflow-timeseries}. After converting wind direction values to radians, $u$ and $v$ were obtained in the following way $$ u = ws * cos(\theta)$$ $$v = ws * sin(\theta)$$ where $ws$ denote wind speed and $\theta$ is the wind direction (in radians). 
\subsubsection{Feature engineering}
In \cref{fig:time_series_plots}, yearly periodicity in the data can be seen, especially for PM$_{10}$ where levels tend to be higher during spring. Daily and weekly periodicity is also expected since traffic intensities vary throughout the day and week. 
%The meteorological variables such as temperature, solar radiation, etc.\ also have periodicity. 
To account for this, timestamps were converted to temporal variables as sine and cosine signals for day, week, and year. For example, the sine and cosine signals for day were calculated in the following way

$$ \text{Sine day = sin} \Big (\text{timestamp} \cdot \frac{2\pi}{86,400} \Big)$$
$$ \text{Cosine day = cos} \Big (\text{timestamp} \cdot \frac{2\pi}{86,400} \Big)$$
where timestamp is in seconds (and with 86,400 seconds in 24 hours, dividing by this term is necessary). The calculations were done similarly for week and year, except for the term in the denominator which instead was set to seconds per week and seconds per year, respectively. 
%The transformations were done so that the sine and cosine functions oscillate between zero and one. 
The temporal variables for day in a 24 hour time window are shown in \vref{fig:time_sine_cos}.

\begin{figure}[h] 
\begin{center}
%\makebox[\textwidth][c]{\includegraphics[width=.585\textwidth]{../plots/time_signals}}
\includegraphics[scale=1.05]{../plots/time_signals}
\caption{Temporal variables for day as sine and cosine signals.}
\label{fig:time_sine_cos}
\end{center}
\end{figure}

\paragraph{Sliding windows}
Sliding windows from the data were also created. The sliding window approach is used for time-series forecasting where windows (or sequences of certain lengths, also called frames) are extracted from the input data \cite{Arsov2021, Gilik2021}. In each window, there are two "sub-windows"; the input window and the target window, and the target window is offset by some amount of time from the input window. For example, as shown in Figure \ref{fig:sliding-window}, the total window length is nine time steps, and the first eight time steps is the input window used to predict the target window (in this case having a length of one) one time step in the future. After extracting a data sequence, the window slides to the right one (or more) steps and extracts the next sequence. This is continued until time step $n$ at which point all the data have been processed. 
In this work, input windows of different lengths were tested to make short-term predictions for a target window with a length of one (more details are given in section \ref{sec:tuning} below). 

 \begin{figure}[h]
\begin{center}
\includegraphics{sliding-windows}
\caption{Sliding window approach for time-series data.}
\label{fig:sliding-window}
\end{center}
\end{figure}

%In this work, input window lengths were set to 12 hr, and predictions were made 1 hr, 6 hr, and 12 hr in the future (giving total window lengths of 13 hr, 18 hr, and 24 hr, respectively). Moreover, single-output models predicting PM levels at one urban background station (Torkel Knutssongatan),  as well as multi-output models predicting PM levels at all urban traffic stations, were tested.

%In this work, the window lengths were set to 12 hours, and both single time-step predictions (target windows of length one) and multi-time-step predictions (target windows of length $n$) were tested. Moreover, prediction models giving both single-outputs (the target value at one monitoring station) as well as multi-outputs (target value at all stations) were constructed.

%\paragraph{Train-test split} Lastly, the data was split into training (60\%), validation (20\%), and test (20\%) sets, where the validation set was used for hyperparameter tuning (described in more detail in section \ref{sec:tuning}). Being sequence data, the sampling was done consecutively, without random shuffling, so that order information would be preserved \cite{Gilik2021}. 
\paragraph{Train-test split} Lastly, the data was split into training, validation, and test sets, where the validation set was used for hyperparameter optimization. The test set was taken as the most recent year of data (from 2019-09-16 to 2020-09-16), the validation set was taken as the year prior to the test data (2018-09-16 to 2019-09-16), and the remaining data was used for training (2016-01-01 to 2018-09-16). This split is motivated by the fact that the data is in the form of time-series, where each observation has a specific time-stamp and where successive observations are (in this case) positively autocorrelated.

%Being sequence data, the sampling was done consecutively, without random shuffling, so that order information would be preserved 

%/Users/simoncarlen/desktop/thesis/data_and_code/plots/PM10_2016_2020_2

%\begin{figure}[h]
%\centering
%\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{../plots/time_series_plots}}
%\caption{Time series plots for PM$_{10}$ and PM$_{2.5}$.}
%\label{fig:time_series_plots}
%\end{figure}

\section{Hyperparameter tuning and model fitting}
\label{sec:tuning}

\subsection{Multiple linear regression models}
Initially, a simple linear regression model was fit with OLS where a value of the response variable at lag one was used as predictor. This model did not display any autocorrelation, but when also including the response variable at lag two as predictor, the Durbin-Watson statistic improved (i.e., was brought closer to 2). Including additional response variables after the first two lags did not lead to further improvements in terms of eliminating autocorrelation. It should be noted that a log transformation of the variables were required to stabilize the variance and also bring the residuals closer to a normal distribution. Even so, deviation from normality was indicated, as can be seen in the residual plots in \cref{fig:residuals_MLR_PM10} in appendix \ref{chapt:appendix_B}. Generally, this is less of a problem if prediction (and not inference) is the sole purpose of a regression analysis, and the central limit theorem also ensures that confidence intervals, t-tests, etc.\ will be increasingly accurate as the sample size increases, even if the distribution of the residuals is not normal \cite{Montgomery2012}. 

Including data (also log transformed) from other stations improved the MLR model, as did including the temporal variables for day, but the temporal variables for week and year were not significant ($p$-values $>$ 0.05). Data from other stations were, similar as for the response variable, entered into the model as the values at lag one (since the values at time $t+1$ cannot be known). Despite having data from several nearby stations, the condition number did not indicate strong multicollinearity. However, when also including weather parameters in the model, the condition number rose considerably. 

% lagged response variables as predictors was fit with OLS. The Durbin-Watson test showed that for both PM$_{10}$ and PM$_{2.5}$, including one response variable ($y_{t-1}$ and $y_{t-2}$) was enough to eliminate any autocorrelation, but 
%With data from three stations in the Stockholm area, some collinearity was expected, and regularized MLR models were initially tried. However, 

\subsection{Deep learning models}

%The Keras Tuner library \cite{omalley2019kerastuner} was used to find the best set of hyperparameters for each model (except for the MLR and ARIMA models used as baseline). 
%%Hyperparameters at both the model architecture-level as well as the input data-level were included in the search space. More specifically, at the input data-level, the width of the windows were set to 3 h, 6 h, or 12 h, and the models were fit with the different versions of the input data. 
%More specifically, the following hyperparameters were tuned:
%
%\begin{itemize}
%\item Number of layers (up to five were tested)
%\item Number of units per layer (in the range [32, 512] with step size set to 32)
%\item Learning rate (sampled uniformly in the range [0.0001, 0.01])
%\item Number of epochs 
%\end{itemize} 
%
%%For some models, a dropout layer (with rates in the range [0, 0.3] and step size set to 0.05) was also tested.
%For the hyperparameter search, Bayesian optimization was used as tuner. (The Bayesian optimization tuner tries to predict which hyperparameters that are likely to improve the model given previous results \cite{omalley2019kerastuner}). The motivation for this choice is the large number of possible hyperparameter combinations, making it infeasible to test all of them within a reasonable amount of time. Instead, it was assumed that the tuner after 75 trials would find some optimal set of hyperparameters. 
%The hyperparameter search was done in total three times for every model; one search each was performed for data input windows of different sizes, namely 8 h, 16 h, and 24 h. After completing the search, the number of epochs for each model were tuned, and all models were re-trained and evaluated on the validation and test data. 

%\begin{figure}[h]
%\centering
%\makebox[\textwidth][c]{\includegraphics[width=0.9\textwidth]{../plots/time_series_plots_7}}
%\caption{Time series plots for PM$_{10}$ and PM$_{2.5}$.}
%\label{fig:time_series_plots}
%\end{figure}

%, the models were re-trained on the training set plus the validation set, and performance on the test set was recorded. Again, with three different window sizes tested, three versions per model were obstained. All predictions were made for PM$_{10}$ one hour ahead for the station at Torkel Knutssongatan (measuring urban background levels in the center of Stockholm).
