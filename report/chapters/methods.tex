%General implementation of the stategy here before going into details about sources, preprocessing, hyperparameter tuning etc, ... Perhaps also a pic?
The major steps of the implemented workflow were as follows; data were retrieved, examined, and preprocessed. Thereafter, a MLR model and several deep neural network models were fit to the data with the appropriate input variables. The models were then used to make forecasts for NO$_2$ with a one-hour forecast horizon. The NO$_2$ forecasts were limited to one station measuring background levels of air pollution in the center of Stockholm (Torkel Knutssonsgatan, see \cref{tab:stations} in Appendix \ref{chapt:appendix_A}). Detailed descriptions of each step in the process are given in subsequent sections. 

The programming language used was Python, together with several libraries for scientific programming, statistics, machine learning, and plotting/visualization (NumPy, pandas, statsmodels, scikit-learn, SciPy, TensorFlow and KerasTuner). All code can be found on the GitHub page\footnote{\url{https://github.com/simoncarlen/forecasting_project}} for the project (invite needed).

%is shown in Figure \ref{fig:dataflow}. Historical air pollution data from several monitoring stations, together with meteorological data from one station, was retrieved, preprocessed (with some features engineered), and divided into data windows. Three deep learning models (feed forward neural network, RNN, and LSTM) were trained and tested for short-term predictions (one hour ahead) of PM$_{10}$ for one station at Torkel Knutssongatan (measuring urban background levels, see Table \ref{tab:stations}). As baseline models for comparison, multiple linear regression and ARIMA were used. Detailed descriptions of each step in the process are given in subsequent sections. 

%\begin{figure}[htbp]
%\begin{center}
%\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{workflow}}
%\caption{Implemented workflow.}
%\label{fig:dataflow}
%\end{center}
%\end{figure}

\section{Data retrieval and preprocessing}
\label{chap:dataprocesschap}

\subsection{Data sources}
\label{sec:data-sources}

Air pollution data was retrieved from the Swedish Meteorological and Hydrological Institute's (SMHI) centralized database for air quality measurements \cite{smhi-luftmatningar}. This data is part of the national and regional environmental monitoring of Sweden, a program coordinated and funded by the Swedish Environmental Protection Agency (Swedish EPA) and the Swedish Agency for Marine and Water Management. There are in total ten different program areas, of which air is one, and all data are licensed under CC0 and therefore freely accessible to the public \cite{naturvardsverket-miljodata}. For the national air monitoring (under Swedish EPA's responsibility), SMHI acts as a national data host and stores (quality checked) historical data reported yearly from municipalities in Sweden \cite{smhi-luftmatningar}.

\subsubsection{Monitoring stations}
In Stockholm County, there are 19 stationary sites for air pollution monitoring \cite{slb-matningar}, and initially, data from each was considered. However, not all sites measure hourly NO$_2$, and for some sites the data were irregular. Therefore, data from four sites with hourly NO$_2$ measurements (in \textmugreek g/m$^3$) for the time period 2016-01-01 to 2022-01-01 was subsequently chosen, giving a total of 52,609 data points. (However, as explained further below, a year worth of data, i.e., 8760 data points were subsequently excluded.) For the station at which NO$_2$ predictions were to be made (Torkel Knutssonsgatan), hourly meteorological data was also utilized. More specifically, these meteorological variables included temperature (in $^\circ$C), precipitation (mm), atmospheric pressure (hPa), relative humidity (as \%), solar radiation (W/m$^2$), and wind speed (m/s). The meteorological data were downloaded from SLB-analys' webpage \cite{slb-analys-meteorologi}. 

In general, air pollution monitoring can be classified by the surrounding area (rural, rural-regional, rural-remote, suburban, and urban), and by the predominant emission sources (background, industrial, or traffic) \cite{smhi-luftmatningar}. The chosen stations included data from both traffic and background monitoring, in urban as well as rural-regional areas. More information about the stations are given in Table \ref{tab:stations} in appendix \ref{chapt:appendix_A}. 

%% STATIONS TABLE
%% ##############
%\begin{table}[]
%\centering
%\caption{Monitoring stations in Stockholm County.}
%\label{tab:stations}
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{@{}llllll@{}}
%\toprule
%Station                         & Station code & Longitude & Latitude  & Type of monitoring                                                   & Parameters                                                                                             \\ \midrule
%NorrtÃ¤lje, Norr Malma           & 18643        & 18.631313 & 59.832382 & \begin{tabular}[c]{@{}l@{}}Rural-Regional \\ Background\end{tabular} & PM$_{10}$, PM$_{2.5}$                                                                                  \\ \midrule
%Stockholm, Hornsgatan 108       & 8780         & 18.04866  & 59.317223 & Urban Traffic                                                        & PM$_{10}$, PM$_{2.5}$                                                                                  \\ \midrule
%Stockholm, Torkel Knutssonsgatan & 8781         & 18.057808 & 59.316006 & Urban background                                                     & \begin{tabular}[c]{@{}l@{}}PM$_{10}$, PM$_{2.5}$, NO$_2$, \\ meteorological \\ parameters\end{tabular} \\ \bottomrule
%\end{tabular}%
%}
%\end{table}
%% ##############

\subsection{Data preprocessing}
\subsubsection{Initial preprocessig}
%(Similar plots for all stations are given in Fig.\ \ref{fig:time_series_plots_all} in appendix \ref{chapt:appendix_A}.)
%(station-wise, with PM$_{10}$ and PM$_{2.5}$ in the left and right subplots, respectively)
All stations had short episodes with missing data, and linear interpolation was used to fill in the missing values. Missing weather data was also linearly interpolated, except atmospheric pressure and wind speed for which mean imputation was deemed more appropriate. Moreover, before use in any of the models, all data were min-max normalized (i.e., scaled to the interval $[0,1]$). 
%However, for the PM$_{2.5}$ data at Torkel Knutssonsgatan (plot (b) in \vref{fig:time_series_plots}), due to the rather big gap at the beginning of 2019,

In \vref{fig:time_series_plots}, the NO$_2$ data for Torkel Knutssonsgatan is shown. A notable reduction in NO$_2$ levels can be seen during 2020 and early 2021; this reduction is most likely due to the COVID-19 pandemic, and by late 2021, pre-pandemic NO$_2$ levels are again approached.  Because of this, a train-test split (see below) was done to entirely avoid using the data from 2020.
% (which was an unusual year with regards to air pollution levels). 
%a train-test split was done to entirely avoid this period (see below). Missing weather data was also linearly interpolated, except for the variables atmospheric pressure and wind speed for which mean imputation was deemed more appropriate. 

%It should be noted that some PM values were negative.
%%this is seen clearly in e.g.\ plot (f) in \vref{fig:time_series_plots}, for the time period Jan.\ 2016 up to about Sep.\ 2019. 
%However, negative values are expected since automated measuring instruments for PM (due to noise) may produce values between zero and the negative detection limit, especially when there are rapid changes in humidity (SMHI, personal communication, April 11, 2022). These values are therefore not to be considered any more "incorrect" than positive values, though it may at first seem contradictory to include them in an analysis. 

\begin{figure}[h]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{../plots/NO2_Torkel_Knutssonsgatan}}
%\includegraphics[width=\textwidth]{../plots/time_series_plots}
%\caption{Time series plots for (a) PM$_{10}$ and (b) PM$_{2.5}$ at Torkel Knutssonsgatan.}
\caption{NO$_2$ data for Torkel Knutssonsgatan.}
\label{fig:time_series_plots}
\end{figure}

%\begin{figure}[h]
%\centering
%\makebox[\textwidth][c]{\includegraphics[width=0.9\textwidth]{../plots/time_series_plots_7}}
%\caption{Time series plots for PM$_{10}$ and PM$_{2.5}$.}
%\label{fig:time_series_plots}
%\end{figure}

%\paragraph{Feature engineering}
%From the meteorological data, wind vectors ($u$ and $v$) were derived from wind direction and wind speed, as wind vectors are more suitable model inputs \cite{tensorflow-timeseries}. After converting wind direction values to radians, $u$ and $v$ were obtained in the following way $$ u = ws * cos(\theta)$$ $$v = ws * sin(\theta)$$ where $ws$ denote wind speed and $\theta$ is the wind direction (in radians). 
\subsubsection{Creating temporal variables}
In \cref{fig:time_series_plots}, yearly periodicity in the data can be seen, where levels tend to peak during winter months. Daily and weekly periodicity is also expected since traffic intensities vary throughout the day and week. 
%The meteorological variables such as temperature, solar radiation, etc.\ also have periodicity. 
To account for this, timestamps were converted to temporal variables as sine and cosine waves for day, week, and year. For example, the sine and cosine waves for day were calculated in the following way

$$ \text{Sine day} = \frac{1}{2} \Big( \text{sin} \Big (\text{timestamp} \cdot \frac{2\pi}{86,400} \Big) + 1 \Big)$$
$$ \text{Cosine day} = \frac{1}{2} \Big( \text{cos} \Big (\text{timestamp} \cdot \frac{2\pi}{86,400} \Big) + 1 \Big)$$
where timestamp is in UNIX epoch time\footnote{The UNIX epoch time for a given timestamp $t$ is the number of seconds that has passed between January 1, 1970, and $t$.} (and with 86,400 seconds in 24 hours, dividing by this term is necessary). The calculations were done similarly for week and year, except for the term in the denominator which instead was set to seconds per week and seconds per year, respectively. Note that the sine and cosine waves were adjusted to oscillate between zero and one. The temporal variables for day in a 24 hour time window are shown in \vref{fig:time_sine_cos}.

\begin{figure}[h] 
\begin{center}
%\makebox[\textwidth][c]{\includegraphics[width=.585\textwidth]{../plots/time_signals}}
\includegraphics[scale=1.05]{../plots/time_signals}
\caption{Temporal variables for day as sine and cosine waves.}
\label{fig:time_sine_cos}
\end{center}
\end{figure}

\subsubsection{Rolling windows}
The rolling windows method extracts data sequences of certain lengths (the "windows") from the input data, and in each window is an "input window" and a "target window" \cite{Gilik2021}. For example, as shown in Figure \ref{fig:sliding-window}  (where $t$ indicate time steps), with a sequence of nine data points, the first eight observations would constitute the input window, and the ninth observation the target window. After extracting this sequence, a slide forward is made to extract the next sequence, and this is continued until observation $n$ becomes the target window, at which point all the data have been processed.% and divided into smaller windows. 

In this work, rolling windows were used as input to the deep learning models, and different input window lengths were tested for making predictions of a target window one time step ahead (i.e., the next hour). More details are given in section \ref{sec:tuning_deep_learning} below.
% for short-term predictions of a target window one step ahead (i.e., the next hour). More details are given in section \ref{sec:tuning_deep_learning} below. 
 \begin{figure}[h]
\begin{center}
%\includegraphics{sliding-windows}
\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{sliding-windows}}
\caption{Rolling window approach for time-series data.}
\label{fig:sliding-window}
\end{center}
\end{figure}
%In this work, input window lengths were set to 12 hr, and predictions were made 1 hr, 6 hr, and 12 hr in the future (giving total window lengths of 13 hr, 18 hr, and 24 hr, respectively). Moreover, single-output models predicting PM levels at one urban background station (Torkel Knutssongatan),  as well as multi-output models predicting PM levels at all urban traffic stations, were tested.
%In this work, the window lengths were set to 12 hours, and both single time-step predictions (target windows of length one) and multi-time-step predictions (target windows of length $n$) were tested. Moreover, prediction models giving both single-outputs (the target value at one monitoring station) as well as multi-outputs (target value at all stations) were constructed.
%\paragraph{Train-test split} Lastly, the data was split into training (60\%), validation (20\%), and test (20\%) sets, where the validation set was used for hyperparameter tuning (described in more detail in section \ref{sec:tuning}). Being sequence data, the sampling was done consecutively, without random shuffling, so that order information would be preserved \cite{Gilik2021}. 
\subsubsection{Train-test split} 
Lastly, the data was split into training, validation, and test sets, where the validation set was used for hyperparameter tuning. The test set was taken as the most recent year of data (from 2021-01-25 to 2022-01-01, where the first 24 days of January were skipped due to many missing values at the Lilla Essingen station). For the validation set, the data from 2019 was used (since 2020 was a year with comparatively low air pollution levels). The remaining data was used for training (2016-01-01 to 2019-01-01). This ordered (as opposed to random) split is motivated by the time dependence in the data. It should also be noted that when normalizing the validation and test sets, min and max values from the training set were used. This ensures that model evaluation will be a good (and not too optimistic) measure of how well the models generalize to new, previously unseen, data points.
%Being sequence data, the sampling was done consecutively, without random shuffling, so that order information would be preserved 

\section{Model fitting and hyperparameter tuning}
\label{sec:model_fitting_hp_tuning}

\subsection{Multiple linear regression models}
Initially, a simple linear regression model was fit with OLS where the response variable at lag one was used as predictor. No significant autocorrelation was seen with this model, but when also including the response variable at lag two as predictor, the Durbin-Watson statistic improved (i.e., was brought closer to 2). Including additional response variables after the first two lags did not lead to further improvements in terms of eliminating autocorrelation.

NO$_2$ data from other stations, meteorological variables, and the temporal variables were subsequently added to the model. These extra predictors did not lead to any serious multicollinearity, as indicated by the condition number. The NO$_2$ data was fit with the values at lag one, since for a forecast at time $t+1$, these predictors cannot be known. However, lagged values of the meteorological variables were not used as these can more easily be replaced with their forecasted values. A similar MLR approach to the one taken here, but for predicting daily means of PM$_{10}$, can be found Stadlober et al. \cite{Stadlober2008}. 

A log transformation of all NO$_2$ data was required before normalization to stabilize the variance of the errors, and also make the error distribution more normal. Even so, deviation from normality was indicated, and as can be seen in \cref{fig:residuals_MLR} in Appendix \ref{chapt:appendix_B} where OLS model diagnostics are shown, the error distribution had long/heavy tails. For this reason, a robust regression model with $M$-estimation (and Huber's $t$ function) was judged to be a more suitable alternative to OLS regression.
%as can be seen in the residual plots in \cref{fig:residuals_MLR} in Appendix \ref{chapt:appendix_B}. More specifically, the error distribution had long tails, which is why robust regression with $M$-estimates (and Huber's function) was judged to be a more suitable alternative to OLS regression. 

At this point, with many input variables in the model, recursive feature elimination (RFE) was used as a variable selection technique, in which the model is repeatedly re-fit after having removed the least significant predictor \cite{Faraway2020}. Each candidate model generated by RFE was evaluated on the validation set (as well as the training set for comparison), and the results are shown in \vref{fig:RFE}. 

\begin{figure}[h] 
\begin{center}
\includegraphics[scale=1.05]{../plots/RMSE_predictors}
\caption{RMSE for each candidate model generated by RFE.}
\label{fig:RFE}
\end{center}
\end{figure}

From \cref{fig:RFE}, it is evident that the least important predictors brought essentially no improvements to the model, and they were therefore removed. More specifically, these variables were; precipitation, atmospheric pressure, the Norr Malma NO$_2$ data, and the temporal variables for week and year. The final MLR model thus had the form
\begin{align}
%\log y_t = \beta_0 + \sum_{\mathclap{\substack{i=1,\\ l\in\{1,2,23,24\}\\}}}^{4} \beta_{i} \log y_{i,t-l} + \sum_{i=5}^{7} \beta_{i} \log x_{i,t-1} + \sum_{i=8}^{11} \beta_{i} z_{i,t}  + \sum_{i=12}^{13} \beta_{i} w_{i,t} +  \varepsilon_t
\log y_t = \beta_0 + \sum_{i=1}^{2} \beta_{i} \log y_{i,t-i} + \sum_{i=3}^{5} \beta_{i} \log x_{i,t-1} + \sum_{i=6}^{9} \beta_{i} z_{i,t}  + \sum_{i=10}^{11} \beta_{i} w_{i,t} +  \varepsilon_t
\label{eq:mlr}
\end{align}
where $x$ denotes input variables with NO$_2$ data from other than the target station, $z$ meteorological variables, $w$ the temporal variables for day, and t = 1, 2, ..., T. Also, the errors ($\varepsilon_t$) in \cref{eq:mlr} are assumed to follow a Gaussian-shaped, heavy-tailed probability distribution (see \cref{fig:residuals_MLR}b in Appendix \ref{chapt:appendix_B}). Summary statistics for this robust regression model, together with values and inference for the estimated parameters, are given in \cref{tab:Robust_table} in Appendix \ref{chapt:appendix_B}. Also, in \cref{tab:OLS_table} in Appendix \ref{chapt:appendix_B}, summary statistics for the OLS regression model are given, though this model was not used to make any forecasts. \\

%\begin{equation}
%\sum_{\mathclap{\substack{i=1,\\ j\in\{1,2,23,24\}\\}}}^{4}
%\end{equation}
%$$
%\sum_{(i,l) \in {(1,1), (2,2), (3, 23), (4,24)}} \betai y_{i, t-l}
%$\sum_{(i,l) \in\{(1,1), (2,2), (3,23), (4,24)\}}  \beta_{i} \log y_{i,t-l} $
%$$
%However, with many input variables in the model, an $L_1$-regularized robust regression was tested. 

%(Summary statistics for the OLS regression is given in Table \ref{tab:OLS_table} in Appendix \ref{chapt:appendix_B}.)

%It should be noted that a log transformation of all NO$_2$ data was required to stabilize the variance of the errors, and also bring them closer to a normal distribution. Even so, deviation from normality was indicated, as can be seen in the residual plots in \cref{fig:residuals_MLR} in Appendix \ref{chapt:appendix_B}. More specifically, the error distribution had long tails, which is why robust regression with $M$-estimates (and Huber's method) was used instead of OLS. Summary statistics for both the OLS and robust regression are given, respectively, in Table \ref{tab:OLS_table} and \ref{tab:Robust_table} in Appendix \ref{chapt:appendix_B}. 

%Some input variables were not significant as indicated by their $p$-values. (These variables were the NO$_2$ data from Norr Malma, precipitation, and temperature.) Removing them lowered the condition number to below 100 and improved the model, as indicated by the $R^2$-value and the RMSE (when making new predictions on the test data). Still, with many input variables, an $L_1$-regularized version of robust regression was tested \cite{Huber2009}. This model shrunk the coefficients somewhat, but only brought minimal improvements in terms of RMSE (less than one tenth of a percent), which is why the non-regularized robust regression model was deemed sufficient as a base model for comparison. The final model had the form 

%With many variables in the model, a regularized approach was tested, namely ridge regression. However, \\

%Including data (also log transformed) from other stations improved the MLR model, as did including the temporal variables for day, but the temporal variables for week and year were not significant ($p$-values $>$ 0.05). Data from other stations were, similar as for the response variable, fit with the values at lag one (since future values of the predictors cannot be known). Despite having data from several nearby stations, the condition number did not indicate any strong multicollinearity. However, when also including weather parameters in the model, the condition number rose to well above 100. Due to this, 

% lagged response variables as predictors was fit with OLS. The Durbin-Watson test showed that for both PM$_{10}$ and PM$_{2.5}$, including one response variable ($y_{t-1}$ and $y_{t-2}$) was enough to eliminate any autocorrelation, but 
%With data from three stations in the Stockholm area, some collinearity was expected, and regularized MLR models were initially tried. However, 

\subsection{Deep neural network models}

\subsubsection{Model fitting}
\label{sec:model_fitting}
At first, a fully connected neural network (dense model) was trained with the same input data as for the MLR model (i.e., the values at lag one and two for the response variable, lag one values for the NO$_2$ data from the other three stations, as well as the meteorological and temporal variables). Subsequent deep learning models, namely an additional dense model, a simple RNN, an LSTM, and a GRU model, were all fit with data windows of different input lengths (6, 12, and 24 hours, with the next hour as the target window), however with the same set of input variables as for the MLR and the first dense model. 

It should be emphasized that these two model fitting strategies differ in that the models with rolling windows utilize multivariate time series from the immediate past, whereas the first dense and the MLR model utilize past pollution data, as well as "real-time" data of the meteorological variables (and hence the meteorological variables would need to be replaced with forecasted values should the models be used in operational mode). Having the same set of input variables (again, being NO$_2$ data from the target site as well as three adjacent sites, four meteorological variables, and the two temporal variables) for all models allows for a more direct comparison between the baseline MLR model and the deep learning models, as well as the two different model fitting strategies. 

\subsubsection{Hyperparameter tuning}
\label{sec:tuning_deep_learning}
For all deep learning models, the hyperparameters tuned were; number of hidden layers (up to five were tested), number of units in each hidden layer (32--512, with step size 32), and learning rate (sampled in the range $[1\times10^{-5},\: 1\times10^{-2}]$). For the two dense models, ReLU was used as activation function, whereas for the RNN, LSTM, and GRU models, the tanh function was used. 

To prevent overfitting, a dropout layer was added after each hidden layer for the dense models, whereas for the simple RNN, LSTM, and GRU models, recurrent dropout were used. For all models, the dropout rate was also tuned (0.0--0.5 with step size 0.1, where a dropout rate of zero is equivalent to no dropout). The simple RNN, LSTM, and GRU models all had a dense output layer, and to regularize this as well, a dropout layer was added before the dense output (where the dropout rate was tuned as described above). 
%If a model showed signs of overfitting (to check this, the learning curves during training were consulted), a drop-out layer was added after each layer, where the dropout rate was also tuned (0.0--0.5, with step size 0.05). Moreover, input windows of different lengths (3, 6, 12, and 24 h) were tested for all models as well. 

Due to the large number of hyperparameter combinations (making it infeasible to test all within a reasonable amount of time), the Bayesian optimization tuner provided by the Keras library was used \cite{omalley2019kerastuner}. This tuner uses Gaussian processes to select hyperparameters that are likely to improve the model given previous results, and it was assumed that convergence to an optimal set of hyperparameters would be found relatively quickly (maximum number of trials were set to 50). 
%Furthermore, the same input variables as for the final MLR model were used in all deep learning models as this provides a more direct comparison of the performance between the two type of models.
As a last step, the optimal number of epochs were tuned as well, and the final models were re-trained with the best set of hyperparameters (including the best epoch), and finally evaluated on the test sets. 

The best set of hyperparameters for each model are given in \cref{tab:hyperparams}. For the models that were fit with rolling windows, the length of the input windows can be viewed as a hyperparameter as well, and in \cref{tab:hyperparams}, only the window length (indicated in parentheses) for the best performing models are given.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h]
\centering
\caption{The best set of hyperparameters for each model. (NA = not applicable.)}
\label{tab:hyperparams}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllll@{}}
\toprule
Model &
  Learning rate &
  \begin{tabular}[c]{@{}l@{}}Units/dropout,\\ 1st layer\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Units/dropout,\\ 2nd layer\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Units/dropout,\\ 3rd layer\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Units/dropout,\\ 4th layer\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Dropout before\\ output layer\end{tabular} \\ \midrule
Dense model      & $1\times10^{-5}$ & 512 / 0.0 & 32 / 0.0 & 512 / 0.3 & 32 / 0.0  & NA  \\
Dense model (6h) & $1\times10^{-5}$ & 512 / 0.0 & -        & -         & -         & NA  \\
RNN model (12h)  & $1\times10^{-5}$ & 32 / 0.0  & 32 / 0.0 & -         & -         & 0.0 \\
LSTM model (12h) & $1\times10^{-4}$ & 32 / 0.0  & 32 / 0.0 & -         & -         & 0.0 \\
GRU (24h)        & $1\times10^{-5}$ & 32 / 0.5  & 32 / 0.0 & 32 / 0.4  & 256 / 0.5 & 0.0 \\ \bottomrule
\end{tabular}%
}
\end{table}

%The Keras Tuner library \cite{omalley2019kerastuner} was used to find the best set of hyperparameters for each model (except for the MLR and ARIMA models used as baseline). 
%%Hyperparameters at both the model architecture-level as well as the input data-level were included in the search space. More specifically, at the input data-level, the width of the windows were set to 3 h, 6 h, or 12 h, and the models were fit with the different versions of the input data. 
%More specifically, the following hyperparameters were tuned:
%
%\begin{itemize}
%\item Number of layers (up to five were tested)
%\item Number of units per layer (in the range [32, 512] with step size set to 32)
%\item Learning rate (sampled uniformly in the range [0.0001, 0.01])
%\item Number of epochs 
%\end{itemize} 
%
%%For some models, a dropout layer (with rates in the range [0, 0.3] and step size set to 0.05) was also tested.
%For the hyperparameter search, Bayesian optimization was used as tuner. (The Bayesian optimization tuner tries to predict which hyperparameters that are likely to improve the model given previous results \cite{omalley2019kerastuner}). The motivation for this choice is the large number of possible hyperparameter combinations, making it infeasible to test all of them within a reasonable amount of time. Instead, it was assumed that the tuner after 75 trials would find some optimal set of hyperparameters. 
%The hyperparameter search was done in total three times for every model; one search each was performed for data input windows of different sizes, namely 8 h, 16 h, and 24 h. After completing the search, the number of epochs for each model were tuned, and all models were re-trained and evaluated on the validation and test data. 

%\begin{figure}[h]
%\centering
%\makebox[\textwidth][c]{\includegraphics[width=0.9\textwidth]{../plots/time_series_plots_7}}
%\caption{Time series plots for PM$_{10}$ and PM$_{2.5}$.}
%\label{fig:time_series_plots}
%\end{figure}

%, the models were re-trained on the training set plus the validation set, and performance on the test set was recorded. Again, with three different window sizes tested, three versions per model were obstained. All predictions were made for PM$_{10}$ one hour ahead for the station at Torkel Knutssongatan (measuring urban background levels in the center of Stockholm).
