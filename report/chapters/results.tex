
\section{Model evaluation}
Forecasts on the original scale of a time series are easier to interpret, and therefore the transformations made before model fitting were reversed to convert back to \textmugreek g/m$^3$. This required inverting the normalization, and also for the MLR model, using exp($\hat{y}$) for the predictions.
\subsection{Common regression metrics}
%In this section, commonly used regression metrics for each model are given and the results are briefly discussed. Further down however (section ??), a more formal comparison of the metrics are carried out with two statistical tests.  
The RMSE, MAE, and ME for the robust MLR model and the neural network models are given in \cref{tab:performance_metrics}. Again, for the neural network models that were fit with rolling windows, the length of the input windows are indicated in parenthesis, and only the best performing models (in terms of lowest RMSE) are included here. 

As can be inferred from \cref{tab:performance_metrics}, the neural network models all had lower RMSE than the baseline MLR model, with the LSTM model having the lowest value (3.10), closely followed by the dense model (3.18) that was not fit with rolling windows (see \vref{sec:model_fitting}). Looking at the MAE, also the LSTM model had the lowest value (1.89), again followed by the dense model (1.93), however, for the three remaining neural network models, the MAE were higher than for the baseline MLR model. 
\begin{table}[h]
\small
\centering
\caption{Performance metrics (best models). }
\label{tab:performance_metrics}
\begin{tabular}{@{}lccc@{}}
\toprule
Model            & \multicolumn{1}{l}{RMSE} & \multicolumn{1}{l}{MAE} & \multicolumn{1}{l}{ME} \\ \midrule
Robust MLR model & 3.51                     & 1.99                    & 0.48                   \\
Dense model      & 3.18                     & 1.93                    & -0.24                  \\
Dense model (6h) & 3.23                     & 2.05                    & -0.32                  \\
Simple RNN model (12h)  & 3.32                     & 2.09                    & -0.05                  \\
LSTM model (12h) & 3.10                     & 1.89                    & -0.08                  \\
GRU model (12h)  & 3.26                     & 2.06                    & -0.02                  \\ \bottomrule
\end{tabular}
\end{table}

\noindent
Though both the RMSE and the MAE measure variability in $e_t(1)$, squaring the errors before averaging (as is done with the MSE) will weight the errors differently than when taking the absolute value. More specifically, the MSE/RMSE penalize large errors more than does the MAE, and the three neural network models having comparatively high MAE's (2.05, 2.09, and 2.06) most likely gave better predictions during episodes with frequent pollution peaks than did the baseline MLR model, even though the MLR model had a lower MAE (1.99). Finally, looking at the ME in \cref{tab:performance_metrics}, all recurrent neural network models showed considerably less bias in the forecast errors than did the two dense models and the baseline MLR model (which had the highest ME of 0.48). Consequently, the recurrent models appeared to be superior in terms of generating less biased forecasts. 

\subsection{Examining the forecast errors}
%As described in \cref{sec:deep_learning}, the RNN, LSTM, and GRU models are all recurrent neural network models. Out of these, the LSTM model showed superior performance over two common evaluation metrics (\cref{tab:performance_metrics}). Similarly, for the two dense models, the one fit with the same input data as for the MLR model had better performance compared to the one fit with rolling windows. Therefore, in the sections below where forecast errors are examined closer, the RNN and GRU models as well as the dense model fit with rolling windows are not analyzed further. Hence, the discussion will be limited to the robust MLR model, the first one of the two dense models, and the LSTM model. 
\subsubsection{Residual autocorrelations}

The Box-Pierce test was used with all models to see if the distribution of the sample ACF for the forecast errors were approximately normal. As described in \cref{sec:auto_errors}, this is equivalent to testing whether the forecast errors are white noise. Using the 50 first autocorrelations, the Box-Pierce test statistic ($Q_{\text{BP}}$) were very large for all models, and the corresponding $p$-values were, for all practical purposes, equal to zero. The results from the Box-Pierce tests are summarized in \cref{tab:boxpierce}.

\begin{table}[h]
\small
\centering
\caption{Results from the Box-Pierce tests.}
\label{tab:boxpierce}
\begin{tabular}{@{}lll@{}}
\toprule
Model                  & $Q_{\text{BP}}$ & $p$-value             \\ \midrule
Robust MLR model       & 1520.07         & $1.91\times10^{-285}$ \\
Dense model            & 866.76          & $2.0\times10^{-149}$  \\
Dense model (6h)       & 607.94          & $6.58\times10^{-97}$  \\
Simple RNN model (12h) & 238.62          & $2.13\times10^{-26}$  \\
LSTM model (12h)       & 315.34          & $3.55\times10^{-40}$  \\
GRU model (12h)        & 504.71          & $2.0\times10^{-76}$   \\ \bottomrule
\end{tabular}
\end{table}

\noindent
QQ-plots of the forecast errors for all models are shown in \cref{fig:qq-plot-errors}. Clearly, the errors were not normally distributed for any of the models, as indicated by the deviations from the straight line. Consequently, from \cref{fig:qq-plot-errors} and \cref{tab:boxpierce}, it can be inferred that the forecast errors were not Gaussian white noise. Instead, the errors appeared to be strongly correlated and non-random. The structure of the errors are discussed next.

\subsubsection{Structure of the forecast errors}
An understanding of the structure of the forecast errors can be given by looking at the scatter plots of observed vs.\ predicted values for each model, shown in \cref{fig:correlations}. Also in \cref{fig:correlations}, the correlation between observed and predicted values are indicated with the Pearson correlation coefficient ($\rho$). If the forecasts were unbiased, the data points would be evenly distributed around the diagonal line. However, as can be seen for all models, the forecasts had tendencies to underestimate NO$_2$ values, and this tendency gets stronger as the NO$_2$ values increase. 

The MLR model had the lowest correlation coefficient ($\rho = 0.916$), and also the strongest tendency to underestimate NO$_2$ values, as indicated by the many data points located below the diagonal line in \cref{fig:correlations}a. Contrary to the neural network models (\cref{fig:correlations}b-f), the MLR model also made some predictions that were strong overestimations. For the neural network models, the correlation coefficients were all higher than for the baseline MLR model, and again, the LSTM model had the best performance for this metric ($\rho=0.934$). Furthermore, the predictions were much less biased, both in the lower ranges of NO$_2$ values, but especially in the higher ranges when compared to the MLR model (which showed strong bias already for observations exceeding 25 \textmugreek g/m$^3$). 
%in the lower ranges of NO$_2$ values compared to the MLR model, which shows strong bias already when observations start to exceed 25 \textmugreek g/m$^3$. In the higher ranges of NO$_2$ observations (say, 50 \textmugreek g/m$^3$ and above), all models tended to make predictions that were too low, though the bias in these ranges was much less severe for the neural network models (\cref{fig:correlations}b-f). 
\begin{figure}[h]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{../plots/qq_pred_errors_all}}
\caption{Normal probability plots for the forecast errors.}
\label{fig:qq-plot-errors}
%\end{figure}
%\begin{figure}[]
%\bigskip
%\centering
\vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{../plots/correlations_all}}
\caption{Scatterplots of observed vs.\ predicted NO$_2$ values.}
\label{fig:correlations}
\end{figure}
\clearpage

\noindent
In \cref{fig:pred_vs_obs}, observed and predicted NO$_2$ levels during a 10 day period in December 2021 are shown for the robust MLR model (\cref{fig:pred_vs_obs}a), and the LSTM model (\cref{fig:pred_vs_obs}b). (Only the best performing deep learning model is compared to the baseline MLR model here, since the forecast errors were similar in character for all models.) From these plots, the structure of the forecast errors also become apparent. For example, it can be seen that both models had difficulties predicting the magnitude, as well as duration, of the large NO$_2$ peak occurring around 7--8 December. However, as is also evident, the forecasts of the LSTM model were still better than those of the MLR model for this specific pollution peak. More accurate forecasts for the LSTM model are also seen for the smaller NO$_2$ peaks (e.g.\ the peaks occuring around 3--4 and 6--7 December). At low NO$_2$ values however, predictions for both models followed the observed NO$_2$ values closely.

%When looking at the predictions for lower NO$_2$ levels, it is not clear whether the LSTM model . 

\begin{figure}[h] 
\begin{center}
%\makebox[\textwidth][c]{\includegraphics[width=1.01\textwidth]{../plots/predictions}}
\includegraphics[scale=1.05]{../plots/predictions}
\caption{Observed and predicted NO$_2$ levels during 10 days in December.}
\label{fig:pred_vs_obs}
\end{center}
\end{figure}

\section{Significance tests for regression metrics}

Knowing that the forecast errors for none of the models were normally distributed, a Kruskall-Wallis test followed by the Dunn's test were used to see if there were any significant difference between the common performance metrics. 

%The largest spread in the data points can be seen for the MLR model (\cref{fig:correlations}a), which also had the lowest correlation coefficient ($\rho = 0.916$). 
%
%
%as well as the highest ME (\cref{tab:performance_metrics}). Contrary to the neural network models, the MLR model also made some predictions that were strongly over-estimated. 
%
%All neural network showed stronger correlation between observed and predicted values than , and also for this metric, the LSTM model showed the best performance ($\rho = 0.934$).

%and the Box-Pierce statistic together with corresponding $p$-values for the robust MLR model as well as the dense and LSTM models are given in \ref{tab:box-pierce}. 


%For all deep learning architectures, the 12 h input window gave better predictions than with the 24 h input window, and in \cref{tab:metrics}, common performance metrics for the best performing deep learning models are summarized together with the corresponding performance for the MLR model.

%From \cref{tab:metrics}, it can be inferred that the dense model had the best performance in terms of MSE/RMSE, closely followed by the LSTM model. The RNN model, however, only had slightly better performance than the (baseline) MLR model. Interestingly, the MAPE follows an entirely different pattern, where the MLR model had the lowest value (20\%), and the LSTM model the highest (29.6\%). This indicates a stronger bias in the predictions by the deep learning models compared to the MLR model. 


%In \cref{fig:correlations}, where observed vs.\ predicted NO$_2$ levels are shown for each model together with the corresponding correlation coefficient, the bias in the predictions for the deep learning models can be seen. For example, looking at plots (b)--(d), there is a consistent pattern of too high predictions (indicated by the unequal distribution of observations around the red line). For the MLR model (\cref{fig:correlations}a), though less biased, the bias goes in the opposite direction, with a tendency to make too low predictions. The correlation coefficients followed the same pattern as MSE/RMSE, with the dense model having the highest $\rho$. 

%In \cref{fig:predictions}, predicted and observed NO$_2$ levels for all models during a 9 day window (1st to 9th of Dec) from the test set are shown. Comparing the MLR model with the deep learning models in \cref{fig:predictions}, it is clear that the MLR model performed worse during pollution peaks (e.g.\ the peak observed around 7--8 of Dec was not predicted very accurately). The model best predicting peaks during this time window appears to be the RNN model, but again the RNN model is clearly biased and also had the lowest $\rho$ among the deep learning models. 


%\begin{figure}[h]
%\centering
%\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{../plots/predictions}}
%\caption{Observed and predicted NO$_2$ for 10 days in Dec 2021.}
%\label{fig:predictions}
%\end{figure}

% sine and cosine for hours important for hourly predictions... but day and year unimportant
